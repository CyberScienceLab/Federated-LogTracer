{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8fd52be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4975253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_cluster\n",
    "import torch_scatter\n",
    "import torch_sparse\n",
    "import torch_spline_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfb48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16abd8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf4f5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(str):\n",
    "\tprint (str + ' ' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b05121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta1-cadets-e3-official.json 2023-05-06 10:00:34\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-cadets-e3-official.json.1 2023-05-06 10:00:44\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-cadets-e3-official.json.2 2023-05-06 10:00:54\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "ta1-cadets-e3-official-2.json 2023-05-06 10:02:48\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-cadets-e3-official-2.json.1 2023-05-06 10:02:58\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "ta1-fivedirections-e3-official-2.json 2023-05-06 10:04:09\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.1 2023-05-06 10:04:18\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.2 2023-05-06 10:04:27\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.3 2023-05-06 10:04:35\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.4 2023-05-06 10:04:44\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.5 2023-05-06 10:04:52\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.6 2023-05-06 10:05:01\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.7 2023-05-06 10:05:09\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.8 2023-05-06 10:05:17\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.9 2023-05-06 10:05:26\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.10 2023-05-06 10:05:34\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.11 2023-05-06 10:05:43\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.12 2023-05-06 10:05:51\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.13 2023-05-06 10:05:59\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.14 2023-05-06 10:06:08\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.15 2023-05-06 10:06:16\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.16 2023-05-06 10:06:25\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.17 2023-05-06 10:06:33\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.18 2023-05-06 10:06:41\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.19 2023-05-06 10:06:50\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.20 2023-05-06 10:06:58\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.21 2023-05-06 10:07:07\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.22 2023-05-06 10:07:15\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.23 2023-05-06 10:07:24\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.24 2023-05-06 10:07:33\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.25 2023-05-06 10:07:43\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.26 2023-05-06 10:07:52\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.27 2023-05-06 10:08:01\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.28 2023-05-06 10:08:09\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.29 2023-05-06 10:08:17\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.30 2023-05-06 10:08:26\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.31 2023-05-06 10:08:34\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.32 2023-05-06 10:08:43\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.33 2023-05-06 10:08:51\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.34 2023-05-06 10:09:00\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.35 2023-05-06 10:09:08\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.36 2023-05-06 10:09:17\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.37 2023-05-06 10:09:25\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.38 2023-05-06 10:09:33\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.39 2023-05-06 10:09:42\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.40 2023-05-06 10:09:50\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.41 2023-05-06 10:09:59\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.42 2023-05-06 10:10:07\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.43 2023-05-06 10:10:15\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.44 2023-05-06 10:10:24\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.45 2023-05-06 10:10:32\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.46 2023-05-06 10:10:40\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.47 2023-05-06 10:10:48\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.48 2023-05-06 10:10:57\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.49 2023-05-06 10:11:05\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.50 2023-05-06 10:11:13\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.51 2023-05-06 10:11:22\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-fivedirections-e3-official-2.json.52 2023-05-06 10:11:31\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "ta1-theia-e3-official-1r.json 2023-05-06 10:47:55\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-1r.json.1 2023-05-06 10:48:03\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-1r.json.2 2023-05-06 10:48:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-1r.json.3 2023-05-06 10:48:20\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-1r.json.4 2023-05-06 10:48:28\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-1r.json.5 2023-05-06 10:48:36\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-1r.json.6 2023-05-06 10:48:44\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-1r.json.7 2023-05-06 10:48:52\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-1r.json.8 2023-05-06 10:49:00\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-1r.json.9 2023-05-06 10:49:08\n",
      "1000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "ta1-theia-e3-official-6r.json 2023-05-06 10:56:27\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.1 2023-05-06 10:56:35\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.2 2023-05-06 10:56:44\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.3 2023-05-06 10:56:52\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.4 2023-05-06 10:57:00\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.5 2023-05-06 10:57:08\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.6 2023-05-06 10:57:17\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.7 2023-05-06 10:57:24\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.8 2023-05-06 10:57:32\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.9 2023-05-06 10:57:41\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.10 2023-05-06 10:57:49\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.11 2023-05-06 10:57:57\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-theia-e3-official-6r.json.12 2023-05-06 10:58:06\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "ta1-trace-e3-official-1.json 2023-05-06 11:07:56\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-trace-e3-official-1.json.1 2023-05-06 11:08:05\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-trace-e3-official-1.json.2 2023-05-06 11:08:14\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-trace-e3-official-1.json.3 2023-05-06 11:08:22\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-trace-e3-official-1.json.4 2023-05-06 11:08:31\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-trace-e3-official-1.json.5 2023-05-06 11:08:40\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "ta1-trace-e3-official-1.json.6 2023-05-06 11:08:47\n",
      "1000000\n",
      "2000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_list = ['ta1-cadets-e3-official.json', 'ta1-cadets-e3-official-2.json', 'ta1-fivedirections-e3-official-2.json', 'ta1-theia-e3-official-1r.json', 'ta1-theia-e3-official-6r.json', 'ta1-trace-e3-official-1.json']\n",
    "\n",
    "# pattern_uuid = re.compile(r'uuid\\\":\\\"(.*?)\\\"') \n",
    "# pattern_src = re.compile(r'subject\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "# pattern_dst1 = re.compile(r'predicateObject\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "# pattern_dst2 = re.compile(r'predicateObject2\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "# pattern_type = re.compile(r'type\\\":\\\"(.*?)\\\"')\n",
    "# pattern_time = re.compile(r'timestampNanos\\\":(.*?),')\n",
    "\n",
    "# notice_num = 1000000\n",
    "\n",
    "# for path in path_list:\n",
    "# \tid_nodetype_map = {}\n",
    "# \tfor i in range(100):\n",
    "# \t\tnow_path  = path + '.' + str(i)\n",
    "# \t\tif i == 0: now_path = path\n",
    "# \t\tif not osp.exists(now_path): break\n",
    "# \t\tf = open(now_path, 'r')\n",
    "# \t\tshow(now_path)\n",
    "# \t\tcnt  = 0\n",
    "# \t\tfor line in f:\n",
    "# \t\t\tcnt += 1\n",
    "# \t\t\tif cnt % notice_num == 0:\n",
    "# \t\t\t\tprint(cnt)\n",
    "# \t\t\tif 'com.bbn.tc.schema.avro.cdm18.Event' in line or 'com.bbn.tc.schema.avro.cdm18.Host' in line: continue\n",
    "# \t\t\tif 'com.bbn.tc.schema.avro.cdm18.TimeMarker' in line or 'com.bbn.tc.schema.avro.cdm18.StartMarker' in line: continue\n",
    "# \t\t\tif 'com.bbn.tc.schema.avro.cdm18.UnitDependency' in line or 'com.bbn.tc.schema.avro.cdm18.EndMarker' in line: continue\n",
    "# \t\t\tif len(pattern_uuid.findall(line)) == 0: print (line)\n",
    "# \t\t\tuuid = pattern_uuid.findall(line)[0]\n",
    "# \t\t\tsubject_type = pattern_type.findall(line)\n",
    "\n",
    "# \t\t\tif len(subject_type) < 1:\n",
    "# \t\t\t\tif 'com.bbn.tc.schema.avro.cdm18.MemoryObject' in line:\n",
    "# \t\t\t\t\tid_nodetype_map[uuid] = 'MemoryObject'\n",
    "# \t\t\t\t\tcontinue\n",
    "# \t\t\t\tif 'com.bbn.tc.schema.avro.cdm18.NetFlowObject' in line:\n",
    "# \t\t\t\t\tid_nodetype_map[uuid] = 'NetFlowObject'\n",
    "# \t\t\t\t\tcontinue\n",
    "# \t\t\t\tif 'com.bbn.tc.schema.avro.cdm18.UnnamedPipeObject' in line:\n",
    "# \t\t\t\t\tid_nodetype_map[uuid] = 'UnnamedPipeObject'\n",
    "# \t\t\t\t\tcontinue\n",
    "\n",
    "# \t\t\tid_nodetype_map[uuid] = subject_type[0]\n",
    "# \tnot_in_cnt = 0\n",
    "# \tfor i in range(100):\n",
    "# \t\tnow_path  = path + '.' + str(i)\n",
    "# \t\tif i == 0: now_path = path\n",
    "# \t\tif not osp.exists(now_path): break\n",
    "# \t\tf = open(now_path, 'r')\n",
    "# \t\tfw = open(now_path+'.txt', 'w')\n",
    "# \t\tcnt = 0\n",
    "# \t\tfor line in f:\n",
    "# \t\t\tcnt += 1\n",
    "# \t\t\tif cnt % notice_num == 0:\n",
    "# \t\t\t\tprint(cnt) \n",
    "\n",
    "# \t\t\tif 'com.bbn.tc.schema.avro.cdm18.Event' in line:\n",
    "# \t\t\t\tpattern = re.compile(r'subject\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "# \t\t\t\tedgeType = pattern_type.findall(line)[0]\n",
    "# \t\t\t\ttimestamp = pattern_time.findall(line)[0]\n",
    "# \t\t\t\tsrcId = pattern_src.findall(line)\n",
    "# \t\t\t\tif len(srcId) == 0: continue\n",
    "# \t\t\t\tsrcId = srcId[0]\n",
    "# \t\t\t\tif not srcId in id_nodetype_map.keys(): \n",
    "# \t\t\t\t\tnot_in_cnt += 1\n",
    "# \t\t\t\t\tcontinue\n",
    "# \t\t\t\tsrcType = id_nodetype_map[srcId]\n",
    "# \t\t\t\tdstId1 = pattern_dst1.findall(line)\n",
    "# \t\t\t\tif len(dstId1) > 0  and dstId1[0] != 'null':\n",
    "# \t\t\t\t\tdstId1 = dstId1[0]\n",
    "# \t\t\t\t\tif not dstId1 in id_nodetype_map.keys():\n",
    "# \t\t\t\t\t\tnot_in_cnt += 1\n",
    "# \t\t\t\t\t\tcontinue\n",
    "# \t\t\t\t\tdstType1 = id_nodetype_map[dstId1]\n",
    "# \t\t\t\t\tthis_edge1 = str(srcId) + '\\t' + str(srcType) + '\\t' + str(dstId1) + '\\t' + str(dstType1) + '\\t' + str(edgeType) + '\\t' + str(timestamp) + '\\n'\n",
    "# \t\t\t\t\tfw.write(this_edge1)\n",
    "\n",
    "# \t\t\t\tdstId2 = pattern_dst2.findall(line)\n",
    "# \t\t\t\tif len(dstId2) > 0  and dstId2[0] != 'null':\n",
    "# \t\t\t\t\tdstId2 = dstId2[0]\n",
    "# \t\t\t\t\tif not dstId2 in id_nodetype_map.keys():\n",
    "# \t\t\t\t\t\tnot_in_cnt += 1\n",
    "# \t\t\t\t\t\tcontinue\n",
    "# \t\t\t\t\tdstType2 = id_nodetype_map[dstId2]\n",
    "# \t\t\t\t\tthis_edge2 = str(srcId) + '\\t' + str(srcType) + '\\t' + str(dstId2) + '\\t' + str(dstType2) + '\\t' + str(edgeType) + '\\t' + str(timestamp) + '\\n'\n",
    "# \t\t\t\t\tfw.write(this_edge2)\t\n",
    "# \t\tfw.close()\n",
    "# \t\tf.close()\n",
    "# os.system('cp ta1-theia-e3-official-1r.json.txt theia_train.txt')\n",
    "# os.system('cp ta1-theia-e3-official-6r.json.8.txt theia_test.txt')\n",
    "# os.system('cp ta1-cadets-e3-official.json.1.txt cadets_train.txt')\n",
    "# os.system('cp ta1-cadets-e3-official-2.json.txt cadets_test.txt')\n",
    "# os.system('cp ta1-fivedirections-e3-official-2.json.txt fivedirections_train.txt')\n",
    "# os.system('cp ta1-fivedirections-e3-official-2.json.23.txt fivedirections_test.txt')\n",
    "# os.system('cp ta1-trace-e3-official-1.json.txt trace_train.txt')\n",
    "# os.system('cp ta1-trace-e3-official-1.json.4.txt trace_test.txt')\n",
    "\n",
    "# # os.system('rm ta1-*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e1c7d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ta1-cadets-e3-official.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-cadets-e3-official.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-cadets-e3-official.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processing file: ta1-cadets-e3-official-2.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-cadets-e3-official-2.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.3\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.4\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.5\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.6\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.7\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.8\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.9\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.10\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.11\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.12\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.13\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.14\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.15\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.16\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.17\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.18\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.19\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.20\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.21\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.22\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.23\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.24\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.25\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.26\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.27\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.28\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.29\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.30\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.31\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.32\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.33\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.34\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.35\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.36\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.37\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.38\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.40\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.41\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.42\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.43\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.44\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.45\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.46\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.47\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.48\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.49\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.50\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.51\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.52\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.3\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.4\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.5\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.6\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.7\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.8\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.9\n",
      "Processed 1000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.3\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.4\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.5\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.6\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.7\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.8\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.9\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.10\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.11\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.12\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.3\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.4\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.5\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.6\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "\n",
    "path_list = ['ta1-cadets-e3-official.json', 'ta1-cadets-e3-official-2.json', 'ta1-fivedirections-e3-official-2.json', 'ta1-theia-e3-official-1r.json', 'ta1-theia-e3-official-6r.json', 'ta1-trace-e3-official-1.json']\n",
    "\n",
    "pattern_uuid = re.compile(r'uuid\\\":\\\"(.*?)\\\"')\n",
    "pattern_src = re.compile(r'subject\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "pattern_dst1 = re.compile(r'predicateObject\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "pattern_dst2 = re.compile(r'predicateObject2\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "pattern_type = re.compile(r'type\\\":\\\"(.*?)\\\"')\n",
    "pattern_time = re.compile(r'timestampNanos\\\":(.*?),')\n",
    "# New pattern for extracting path information\n",
    "pattern_path = re.compile(r'\"predicateObjectPath\":\\{\"string\":\"(.*?)\"\\}')\n",
    "# pattern_path2 = re.compile(r'\"predicateObject2Path\":\\{\"string\":\"(.*?)\"\\}')\n",
    "\n",
    "\n",
    "notice_num = 1000000\n",
    "\n",
    "for path in path_list:\n",
    "    id_nodetype_map = {}\n",
    "    for i in range(100):\n",
    "        now_path = path + '.' + str(i)\n",
    "        if i == 0: now_path = path\n",
    "        if not osp.exists(now_path): break\n",
    "        with open(now_path, 'r') as f:\n",
    "            print(f\"Processing file: {now_path}\")\n",
    "            cnt = 0\n",
    "            for line in f:\n",
    "                cnt += 1\n",
    "                if cnt % notice_num == 0:\n",
    "                    print(f\"Processed {cnt} lines\")\n",
    "                if 'com.bbn.tc.schema.avro.cdm18.Event' in line or 'com.bbn.tc.schema.avro.cdm18.Host' in line: continue\n",
    "                if 'com.bbn.tc.schema.avro.cdm18.TimeMarker' in line or 'com.bbn.tc.schema.avro.cdm18.StartMarker' in line: continue\n",
    "                if 'com.bbn.tc.schema.avro.cdm18.UnitDependency' in line or 'com.bbn.tc.schema.avro.cdm18.EndMarker' in line: continue\n",
    "                if len(pattern_uuid.findall(line)) == 0:\n",
    "                    print(line)\n",
    "                    continue\n",
    "                uuid = pattern_uuid.findall(line)[0]\n",
    "                subject_type = pattern_type.findall(line)\n",
    "\n",
    "                if len(subject_type) < 1:\n",
    "                    if 'com.bbn.tc.schema.avro.cdm18.MemoryObject' in line:\n",
    "                        id_nodetype_map[uuid] = 'MemoryObject'\n",
    "                        continue\n",
    "                    if 'com.bbn.tc.schema.avro.cdm18.NetFlowObject' in line:\n",
    "                        id_nodetype_map[uuid] = 'NetFlowObject'\n",
    "                        continue\n",
    "                    if 'com.bbn.tc.schema.avro.cdm18.UnnamedPipeObject' in line:\n",
    "                        id_nodetype_map[uuid] = 'UnnamedPipeObject'\n",
    "                        continue\n",
    "\n",
    "                id_nodetype_map[uuid] = subject_type[0]\n",
    "\n",
    "    not_in_cnt = 0\n",
    "    for i in range(100):\n",
    "        now_path = path + '.' + str(i)\n",
    "        if i == 0: now_path = path\n",
    "        if not osp.exists(now_path): break\n",
    "        with open(now_path, 'r') as f, open(now_path + '.txt', 'w') as fw:\n",
    "            cnt = 0\n",
    "            for line in f:\n",
    "                cnt += 1\n",
    "                if cnt % notice_num == 0:\n",
    "                    print(f\"Processed {cnt} lines\")\n",
    "\n",
    "                if 'com.bbn.tc.schema.avro.cdm18.Event' in line:\n",
    "                    edgeType = pattern_type.findall(line)[0]\n",
    "                    timestamp = pattern_time.findall(line)[0]\n",
    "                    srcId = pattern_src.findall(line)\n",
    "                    if len(srcId) == 0: continue\n",
    "                    srcId = srcId[0]\n",
    "                    if srcId not in id_nodetype_map.keys():\n",
    "                        not_in_cnt += 1\n",
    "                        continue\n",
    "                    srcType = id_nodetype_map[srcId]\n",
    "                    dstId1 = pattern_dst1.findall(line)\n",
    "                    path_info = pattern_path.findall(line)\n",
    "                    path_info = path_info[0] if path_info else \"N/A\"\n",
    "                    if len(dstId1) > 0 and dstId1[0] != 'null':\n",
    "                        dstId1 = dstId1[0]\n",
    "                        if dstId1 not in id_nodetype_map.keys():\n",
    "                            not_in_cnt += 1\n",
    "                            continue\n",
    "                        dstType1 = id_nodetype_map[dstId1]\n",
    "                        this_edge1 = f\"{srcId}\\t{srcType}\\t{dstId1}\\t{dstType1}\\t{edgeType}\\t{timestamp}\\t{path_info}\\n\"\n",
    "                        fw.write(this_edge1)\n",
    "\n",
    "                    dstId2 = pattern_dst2.findall(line)\n",
    "                    if len(dstId2) > 0 and dstId2[0] != 'null':\n",
    "                        dstId2 = dstId2[0]\n",
    "                        if dstId2 not in id_nodetype_map.keys():\n",
    "                            not_in_cnt += 1\n",
    "                            continue\n",
    "                        dstType2 = id_nodetype_map[dstId2]\n",
    "                        this_edge2 = f\"{srcId}\\t{srcType}\\t{dstId2}\\t{dstType2}\\t{edgeType}\\t{timestamp}\\t{path_info}\\n\"\n",
    "                        fw.write(this_edge2)         \n",
    "            fw.close()\n",
    "            f.close()\n",
    "os.system('cp ta1-theia-e3-official-1r.json.txt theia_train.txt')\n",
    "os.system('cp ta1-theia-e3-official-6r.json.8.txt theia_test.txt')\n",
    "os.system('cp ta1-cadets-e3-official.json.1.txt cadets_train.txt')\n",
    "os.system('cp ta1-cadets-e3-official-2.json.txt cadets_test.txt')\n",
    "os.system('cp ta1-fivedirections-e3-official-2.json.txt fivedirections_train.txt')\n",
    "os.system('cp ta1-fivedirections-e3-official-2.json.23.txt fivedirections_test.txt')\n",
    "os.system('cp ta1-trace-e3-official-1.json.txt trace_train.txt')\n",
    "os.system('cp ta1-trace-e3-official-1.json.4.txt trace_test.txt')\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e51652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ta1-cadets-e3-official.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-cadets-e3-official.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-cadets-e3-official.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processing file: ta1-cadets-e3-official-2.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-cadets-e3-official-2.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.3\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.4\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.5\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.6\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.7\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.8\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.9\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.10\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.11\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.12\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.13\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.14\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.15\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.16\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.17\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.18\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.19\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.20\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.21\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.22\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.23\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.24\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.25\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.26\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.27\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.28\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.29\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.30\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.31\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.32\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.33\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.34\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.35\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.36\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.37\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.38\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.40\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.41\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.42\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.43\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.44\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.45\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.46\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.47\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.48\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.49\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.50\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.51\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-fivedirections-e3-official-2.json.52\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.3\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.4\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.5\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.6\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.7\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.8\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-1r.json.9\n",
      "Processed 1000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.3\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.4\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.5\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.6\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.7\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.8\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.9\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.10\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.11\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-theia-e3-official-6r.json.12\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.1\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.2\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.3\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.4\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.5\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processing file: ta1-trace-e3-official-1.json.6\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n",
      "Processed 3000000 lines\n",
      "Processed 4000000 lines\n",
      "Processed 5000000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 2000000 lines\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import re\n",
    "# import os\n",
    "# import os.path as osp\n",
    "\n",
    "\n",
    "# path_list = ['ta1-cadets-e3-official.json', 'ta1-cadets-e3-official-2.json', 'ta1-fivedirections-e3-official-2.json', 'ta1-theia-e3-official-1r.json', 'ta1-theia-e3-official-6r.json', 'ta1-trace-e3-official-1.json']\n",
    "\n",
    "# pattern_uuid = re.compile(r'uuid\\\":\\\"(.*?)\\\"')\n",
    "# pattern_src = re.compile(r'subject\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "# pattern_dst1 = re.compile(r'predicateObject\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "# pattern_dst2 = re.compile(r'predicateObject2\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "# pattern_type = re.compile(r'type\\\":\\\"(.*?)\\\"')\n",
    "# pattern_time = re.compile(r'timestampNanos\\\":(.*?),')\n",
    "# # New pattern for extracting path information\n",
    "# pattern_path = re.compile(r'\"predicateObjectPath\":\\{\"string\":\"(.*?)\"\\}')\n",
    "# pattern_path2= re.compile(r'\"predicateObject2Path\":\\{\"string\":\"(.*?)\"\\}')\n",
    "\n",
    "\n",
    "# notice_num = 1000000\n",
    "\n",
    "# for path in path_list:\n",
    "#     id_nodetype_map = {}\n",
    "#     for i in range(100):\n",
    "#         now_path = path + '.' + str(i)\n",
    "#         if i == 0: now_path = path\n",
    "#         if not osp.exists(now_path): break\n",
    "#         with open(now_path, 'r') as f:\n",
    "#             print(f\"Processing file: {now_path}\")\n",
    "#             cnt = 0\n",
    "#             for line in f:\n",
    "#                 cnt += 1\n",
    "#                 if cnt % notice_num == 0:\n",
    "#                     print(f\"Processed {cnt} lines\")\n",
    "#                 if 'com.bbn.tc.schema.avro.cdm18.Event' in line or 'com.bbn.tc.schema.avro.cdm18.Host' in line: continue\n",
    "#                 if 'com.bbn.tc.schema.avro.cdm18.TimeMarker' in line or 'com.bbn.tc.schema.avro.cdm18.StartMarker' in line: continue\n",
    "#                 if 'com.bbn.tc.schema.avro.cdm18.UnitDependency' in line or 'com.bbn.tc.schema.avro.cdm18.EndMarker' in line: continue\n",
    "#                 if len(pattern_uuid.findall(line)) == 0:\n",
    "#                     print(line)\n",
    "#                     continue\n",
    "#                 uuid = pattern_uuid.findall(line)[0]\n",
    "#                 subject_type = pattern_type.findall(line)\n",
    "\n",
    "#                 if len(subject_type) < 1:\n",
    "#                     if 'com.bbn.tc.schema.avro.cdm18.MemoryObject' in line:\n",
    "#                         id_nodetype_map[uuid] = 'MemoryObject'\n",
    "#                         continue\n",
    "#                     if 'com.bbn.tc.schema.avro.cdm18.NetFlowObject' in line:\n",
    "#                         id_nodetype_map[uuid] = 'NetFlowObject'\n",
    "#                         continue\n",
    "#                     if 'com.bbn.tc.schema.avro.cdm18.UnnamedPipeObject' in line:\n",
    "#                         id_nodetype_map[uuid] = 'UnnamedPipeObject'\n",
    "#                         continue\n",
    "\n",
    "#                 id_nodetype_map[uuid] = subject_type[0]\n",
    "\n",
    "#     not_in_cnt = 0\n",
    "#     for i in range(100):\n",
    "#         now_path = path + '.' + str(i)\n",
    "#         if i == 0: now_path = path\n",
    "#         if not osp.exists(now_path): break\n",
    "#         with open(now_path, 'r') as f, open(now_path + '.txt', 'w') as fw:\n",
    "#             cnt = 0\n",
    "#             for line in f:\n",
    "#                 cnt += 1\n",
    "#                 if cnt % notice_num == 0:\n",
    "#                     print(f\"Processed {cnt} lines\")\n",
    "\n",
    "#                 if 'com.bbn.tc.schema.avro.cdm18.Event' in line:\n",
    "#                     edgeType = pattern_type.findall(line)[0]\n",
    "#                     timestamp = pattern_time.findall(line)[0]\n",
    "#                     srcId = pattern_src.findall(line)\n",
    "#                     if len(srcId) == 0: continue\n",
    "#                     srcId = srcId[0]\n",
    "#                     if srcId not in id_nodetype_map.keys():\n",
    "#                         not_in_cnt += 1\n",
    "#                         continue\n",
    "#                     srcType = id_nodetype_map[srcId]\n",
    "#                     dstId1 = pattern_dst1.findall(line)\n",
    "#                     dstId2 = pattern_dst2.findall(line)\n",
    "#                     path_info = pattern_path.findall(line)\n",
    "#                     path_info2 = pattern_path2.findall(line)\n",
    "#                     path_info = path_info[0] if path_info else \"N/A\"\n",
    "#                     path_info2 = path_info2[0] if path_info2 else \"N/A\"\n",
    "                    \n",
    "#                     if len(dstId1) > 0 and dstId1[0] != 'null':\n",
    "#                         dstId1 = dstId1[0]\n",
    "#                         if dstId1 not in id_nodetype_map.keys():\n",
    "#                             not_in_cnt += 1\n",
    "#                             continue\n",
    "#                         dstType1 = id_nodetype_map[dstId1]\n",
    "#                         this_edge1 = f\"{srcId}\\t{srcType}\\t{dstId1}\\t{dstType1}\\t{edgeType}\\t{timestamp}\\t{path_info}\\n\"\n",
    "#                         fw.write(this_edge1)\n",
    "\n",
    "#                     if len(dstId2) > 0 and dstId2[0] != 'null':\n",
    "#                         dstId2 = dstId2[0]\n",
    "#                         if dstId2 not in id_nodetype_map.keys():\n",
    "#                             not_in_cnt += 1\n",
    "#                             continue\n",
    "#                         dstType2 = id_nodetype_map[dstId2]\n",
    "#                         this_edge2 = f\"{srcId}\\t{srcType}\\t{dstId2}\\t{dstType2}\\t{edgeType}\\t{timestamp}\\t{path_info2}\\n\"\n",
    "#                         fw.write(this_edge2)  \n",
    "#             fw.close()\n",
    "#             f.close()\n",
    "# os.system('cp ta1-theia-e3-official-1r.json.txt theia_train.txt')\n",
    "# os.system('cp ta1-theia-e3-official-6r.json.8.txt theia_test.txt')\n",
    "# os.system('cp ta1-cadets-e3-official.json.1.txt cadets_train.txt')\n",
    "# os.system('cp ta1-cadets-e3-official-2.json.txt cadets_test.txt')\n",
    "# os.system('cp ta1-fivedirections-e3-official-2.json.txt fivedirections_train.txt')\n",
    "# os.system('cp ta1-fivedirections-e3-official-2.json.23.txt fivedirections_test.txt')\n",
    "# os.system('cp ta1-trace-e3-official-1.json.txt trace_train.txt')\n",
    "# os.system('cp ta1-trace-e3-official-1.json.4.txt trace_test.txt')\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a499ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up\n",
    "import os\n",
    "\n",
    "#step1: define environment variable GRAPHCHI_ROOT\n",
    "#############################################\n",
    "graphchi_root = os.path.abspath(os.path.join(os.getcwd(), 'graphchi-cpp-master'))\n",
    "os.environ['GRAPHCHI_ROOT'] = graphchi_root\n",
    "\n",
    "\n",
    "\n",
    "#step2: clean models directory ../models\n",
    "#############################################\n",
    "model_dir = 'models'\n",
    "models = os.listdir(model_dir)\n",
    "# for i in models:\n",
    "# \tpath = os.path.join(model_dir,i)\n",
    "# \tos.system('rm ' + path)\n",
    "# os.system('rm result_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7411dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23c0b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca08c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.data import NeighborSampler, DataLoader\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377ac20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process_train import *\n",
    "from data_process_test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaa7ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_process_train\n",
    "import data_process_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391c005",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c738dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GAE, VGAE\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "def show(str):\n",
    "\tprint (str + ' ' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time())))\n",
    "\n",
    "class TestDatasetA(InMemoryDataset):\n",
    "\tdef __init__(self, data_list):\n",
    "\t\tsuper(TestDatasetA, self).__init__('/tmp/TestDataset')\n",
    "\t\tself.data, self.slices = self.collate(data_list)\n",
    "\n",
    "\tdef _download(self):\n",
    "\t\tpass\n",
    "\tdef _process(self):\n",
    "\t\tpass\n",
    "\n",
    "def MyDatasetA(path, model):\n",
    "\tgraphId = model\n",
    "\tfeature_num = 0\n",
    "\tlabel_num = 0\n",
    "\tf_feature = open('models/feature.txt', 'r')\n",
    "\tfeature_map = {}\n",
    "\tfor i in f_feature:\n",
    "\t\ttemp = i.strip('\\n').split('\\t')\n",
    "\t\tfeature_map[temp[0]] = int(temp[1])\n",
    "\t\tfeature_num += 1\n",
    "\tf_feature.close()\n",
    "\n",
    "\tf_label = open('models/label.txt', 'r')\n",
    "\tlabel_map = {}\n",
    "\tfor i in f_label:\n",
    "\t\ttemp = i.strip('\\n').split('\\t')\n",
    "\t\tlabel_map[temp[0]] = int(temp[1])\n",
    "\t\tlabel_num += 1\n",
    "\tf_label.close()\n",
    "\n",
    "\tf_gt = open('groundtruth_uuid.txt', 'r')\n",
    "\tground_truth = {}\n",
    "\tfor line in f_gt:\n",
    "\t\tground_truth[line.strip('\\n')] = 1\n",
    "\n",
    "\tf_gt.close()\n",
    "\tnode_cnt = 0\n",
    "\tnodeType_cnt = 0\n",
    "\tedgeType_cnt = 0\n",
    "\tprovenance = []\n",
    "\tnodeType_map = {}\n",
    "\tedgeType_map = {}\n",
    "\tedge_s = []\n",
    "\tedge_e = []\n",
    "\tadj = {}\n",
    "\tadj2 = {}\n",
    "\tdata_thre = 1000000\n",
    "\tfw = open('groundtruth_nodeId.txt', 'w')\n",
    "\tfw2 = open('id_to_uuid.txt', 'w')\n",
    "\tnodeId_map = {}\n",
    "\tcnt = 0\n",
    "\tnodeA = []\n",
    "\tfor i in range(1):\n",
    "\t\tnow_path = path\n",
    "\t\tshow(now_path)\n",
    "\t\tf = open(now_path, 'r')\n",
    "\t\tfor line in f:\n",
    "\t\t\tcnt += 1\n",
    "\t\t\ttemp = line.strip('\\n').split('\\t')\n",
    "\t\t\tif not (temp[1] in label_map.keys()): continue\n",
    "\t\t\tif not (temp[3] in label_map.keys()): continue\n",
    "\t\t\tif not (temp[4] in feature_map.keys()): continue\n",
    "\n",
    "\t\t\tif not (temp[0] in nodeId_map.keys()):\n",
    "\t\t\t\tnodeId_map[temp[0]] = node_cnt\n",
    "\t\t\t\tfw2.write(str(node_cnt) + ' ' + temp[0] + '\\n')\n",
    "\n",
    "\t\t\t\tif temp[0] in ground_truth.keys():\n",
    "\t\t\t\t\tfw.write(str(nodeId_map[temp[0]])+' '+temp[1]+' '+temp[0]+'\\n')\n",
    "\t\t\t\t\tnodeA.append(node_cnt)\n",
    "\t\t\t\tnode_cnt += 1\n",
    "\n",
    "\t\t\ttemp[0] = nodeId_map[temp[0]]\t\n",
    "\n",
    "\t\t\tif not (temp[2] in nodeId_map.keys()):\n",
    "\t\t\t\tnodeId_map[temp[2]] = node_cnt\n",
    "\t\t\t\tfw2.write(str(node_cnt) + ' ' + temp[2] + '\\n')\n",
    "\n",
    "\t\t\t\tif temp[2] in ground_truth.keys():\n",
    "\t\t\t\t\tfw.write(str(nodeId_map[temp[2]])+' '+temp[3]+' '+temp[2]+'\\n')\n",
    "\t\t\t\t\tnodeA.append(node_cnt)\n",
    "\t\t\t\tnode_cnt += 1\n",
    "\t\t\ttemp[2] = nodeId_map[temp[2]]\t\t\n",
    "\t\t\ttemp[1] = label_map[temp[1]]\n",
    "\t\t\ttemp[3] = label_map[temp[3]]\n",
    "\t\t\ttemp[4] = feature_map[temp[4]]\n",
    "\t\t\tedge_s.append(temp[0])\n",
    "\t\t\tedge_e.append(temp[2])\n",
    "\t\t\tif temp[2] in adj.keys():\n",
    "\t\t\t\tadj[temp[2]].append(temp[0])\n",
    "\t\t\telse:\n",
    "\t\t\t\tadj[temp[2]] = [temp[0]]\n",
    "\t\t\tif temp[0] in adj2.keys():\n",
    "\t\t\t\tadj2[temp[0]].append(temp[2])\n",
    "\t\t\telse:\n",
    "\t\t\t\tadj2[temp[0]] = [temp[2]]\n",
    "\t\t\tprovenance.append(temp)\n",
    "\t\tf.close()\n",
    "\tfw.close()\n",
    "\tfw2.close()\n",
    "\tx_list = []\n",
    "\ty_list = []\n",
    "\ttrain_mask = []\n",
    "\ttest_mask = []\n",
    "\tfor i in range(node_cnt):\n",
    "\t\ttemp_list = []\n",
    "\t\tfor j in range(feature_num*2):\n",
    "\t\t\ttemp_list.append(0)\n",
    "\t\tx_list.append(temp_list)\n",
    "\t\ty_list.append(0)\n",
    "\t\ttrain_mask.append(True)\n",
    "\t\ttest_mask.append(True)\n",
    "\n",
    "\tfor temp in provenance:\n",
    "\t\tsrcId = temp[0]\n",
    "\t\tsrcType = temp[1]\n",
    "\t\tdstId = temp[2]\n",
    "\t\tdstType = temp[3]\n",
    "\t\tedge = temp[4]\n",
    "\t\tx_list[srcId][edge] += 1\n",
    "\t\ty_list[srcId] = srcType\n",
    "\t\tx_list[dstId][edge+feature_num] += 1\n",
    "\t\ty_list[dstId] = dstType\n",
    "\n",
    "\tx = torch.tensor(x_list, dtype=torch.float)\t\n",
    "\ty = torch.tensor(y_list, dtype=torch.long)\n",
    "\ttrain_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "\ttest_mask = torch.tensor(test_mask, dtype=torch.bool)\n",
    "\tedge_index = torch.tensor([edge_s, edge_e], dtype=torch.long)\n",
    "\tdata1 = Data(x=x, y=y,edge_index=edge_index, train_mask=train_mask, test_mask = test_mask)\n",
    "\tfeature_num *= 2\n",
    "\tneibor = set()\n",
    "\t_neibor = {}\n",
    "\tfor i in nodeA:\n",
    "\t\tneibor.add(i)\n",
    "\t\tif not i in _neibor.keys():\n",
    "\t\t\ttempl = []\n",
    "\t\t\t_neibor[i] = templ\n",
    "\t\tif not i in _neibor[i]:\n",
    "\t\t\t_neibor[i].append(i)\t\t\n",
    "\t\tif i in adj.keys():\n",
    "\t\t\tfor j in adj[i]:\n",
    "\t\t\t\tneibor.add(j)\n",
    "\t\t\t\tif not j in _neibor.keys():\n",
    "\t\t\t\t\ttempl = []\n",
    "\t\t\t\t\t_neibor[j] = templ\n",
    "\t\t\t\tif not i in _neibor[j]:\n",
    "\t\t\t\t\t_neibor[j].append(i)\t\n",
    "\t\t\t\tif not j in adj.keys(): continue\n",
    "\t\t\t\tfor k in adj[j]:\n",
    "\t\t\t\t\tneibor.add(k)\n",
    "\t\t\t\t\tif not k in _neibor.keys():\n",
    "\t\t\t\t\t\ttempl = []\n",
    "\t\t\t\t\t\t_neibor[k] = templ\n",
    "\t\t\t\t\tif not i in _neibor[k]:\n",
    "\t\t\t\t\t\t_neibor[k].append(i)\n",
    "\t\tif i in adj2.keys():\n",
    "\t\t\tfor j in adj2[i]:\n",
    "\t\t\t\tneibor.add(j)\n",
    "\t\t\t\tif not j in adj2.keys(): continue\n",
    "\t\t\t\tfor k in adj2[j]:\n",
    "\t\t\t\t\tneibor.add(k)\n",
    "\t_nodeA = []\n",
    "\tfor i in neibor:\n",
    "\t\t_nodeA.append(i)\n",
    "\treturn [data1], feature_num, label_num, adj, adj2, nodeA, _nodeA, _neibor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df8989a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GAE, VGAE\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "class TestDataset(InMemoryDataset):\n",
    "\tdef __init__(self, data_list):\n",
    "\t\tsuper(TestDataset, self).__init__('/tmp/TestDataset')\n",
    "\t\tself.data, self.slices = self.collate(data_list)\n",
    "\n",
    "\tdef _download(self):\n",
    "\t\tpass\n",
    "\tdef _process(self):\n",
    "\t\tpass\n",
    "\n",
    "def MyDataset(path, model):\n",
    "\tgraphId = model\n",
    "\tnode_cnt = 0\n",
    "\tnodeType_cnt = 0\n",
    "\tedgeType_cnt = 0\n",
    "\tprovenance = []\n",
    "\tnodeType_map = {}\n",
    "\tedgeType_map = {}\n",
    "\tedge_s = []\n",
    "\tedge_e = []\n",
    "\tdata_thre = 1000000\n",
    "\n",
    "\tfor out_loop in range(1):\n",
    "\t\tf = open(path, 'r')\n",
    "\n",
    "\t\tnodeId_map = {}\n",
    "\n",
    "\t\tfor line in f:\n",
    "\t\t\ttemp = line.strip('\\n').split('\\t')\n",
    "\t\t\tif not (temp[0] in nodeId_map.keys()):\n",
    "\t\t\t\tnodeId_map[temp[0]] = node_cnt\n",
    "\t\t\t\tnode_cnt += 1\n",
    "\t\t\ttemp[0] = nodeId_map[temp[0]]\t\n",
    "\n",
    "\t\t\tif not (temp[2] in nodeId_map.keys()):\n",
    "\t\t\t\tnodeId_map[temp[2]] = node_cnt\n",
    "\t\t\t\tnode_cnt += 1\n",
    "\t\t\ttemp[2] = nodeId_map[temp[2]]\n",
    "\n",
    "\t\t\tif not (temp[1] in nodeType_map.keys()):\n",
    "\t\t\t\tnodeType_map[temp[1]] = nodeType_cnt\n",
    "\t\t\t\tnodeType_cnt += 1\n",
    "\t\t\ttemp[1] = nodeType_map[temp[1]]\n",
    "\n",
    "\t\t\tif not (temp[3] in nodeType_map.keys()):\n",
    "\t\t\t\tnodeType_map[temp[3]] = nodeType_cnt\n",
    "\t\t\t\tnodeType_cnt += 1\n",
    "\t\t\ttemp[3] = nodeType_map[temp[3]]\n",
    "\t\t\t\n",
    "\t\t\tif not (temp[4] in edgeType_map.keys()):\n",
    "\t\t\t\tedgeType_map[temp[4]] = edgeType_cnt\n",
    "\t\t\t\tedgeType_cnt += 1\n",
    "\n",
    "\t\t\ttemp[4] = edgeType_map[temp[4]]\n",
    "\t\t\tedge_s.append(temp[0])\n",
    "\t\t\tedge_e.append(temp[2])\n",
    "\t\t\tprovenance.append(temp)\n",
    "\n",
    "\tf_train_feature = open('models/feature.txt', 'w')\n",
    "\tfor i in edgeType_map.keys():\n",
    "\t\tf_train_feature.write(str(i)+'\\t'+str(edgeType_map[i])+'\\n')\n",
    "\tf_train_feature.close()\n",
    "\tf_train_label = open('models/label.txt', 'w')\n",
    "\tfor i in nodeType_map.keys():\n",
    "\t\tf_train_label.write(str(i)+'\\t'+str(nodeType_map[i])+'\\n')\n",
    "\tf_train_label.close()\n",
    "\tfeature_num = edgeType_cnt\n",
    "\tlabel_num = nodeType_cnt\n",
    "\n",
    "\tx_list = []\n",
    "\ty_list = []\n",
    "\ttrain_mask = []\n",
    "\ttest_mask = []\n",
    "\tfor i in range(node_cnt):\n",
    "\t\ttemp_list = []\n",
    "\t\tfor j in range(feature_num*2):\n",
    "\t\t\ttemp_list.append(0)\n",
    "\t\tx_list.append(temp_list)\n",
    "\t\ty_list.append(0\n",
    "                      \n",
    "                      \n",
    "\t\ttrain_mask.append(True)\n",
    "\t\ttest_mask.append(True)\n",
    "\tfor temp in provenance:\n",
    "\t\tsrcId = temp[0]\n",
    "\t\tsrcType = temp[1]\n",
    "\t\tdstId = temp[2]\n",
    "\t\tdstType = temp[3]\n",
    "\t\tedge = temp[4]\n",
    "\t\tx_list[srcId][edge] += 1\n",
    "\t\ty_list[srcId] = srcType\n",
    "\t\tx_list[dstId][edge+feature_num] += 1\n",
    "\t\ty_list[dstId] = dstType\n",
    "\n",
    "\tx = torch.tensor(x_list, dtype=torch.float)\t\n",
    "\ty = torch.tensor(y_list, dtype=torch.long)\n",
    "\ttrain_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "\ttest_mask = train_mask\n",
    "\tedge_index = torch.tensor([edge_s, edge_e], dtype=torch.long)\n",
    "\tdata1 = Data(x=x, y=y,edge_index=edge_index, train_mask=train_mask, test_mask = test_mask)\n",
    "\tfeature_num *= 2\n",
    "\treturn [data1], feature_num, label_num,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a9769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a3aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7951b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8219e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abcadf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import NeighborSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d705e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thre_map = {\"cadets\":1.5,\"trace\":1.0,\"theia\":1.5,\"fivedirections\":1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a51a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "efcaa800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(*s):\n",
    "\tfor i in range(len(s)):\n",
    "\t\tprint (str(s[i]) + ' ', end = '')\n",
    "\tprint (time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time())))\n",
    "\n",
    "####################Original######################################\n",
    "# class SAGENet(torch.nn.Module):\n",
    "# \tdef __init__(self, in_channels, out_channels, concat=False):\n",
    "# \t\tsuper(SAGENet, self).__init__()\n",
    "# \t\t#self.conv1 = SAGEConv(in_channels, 32, normalize=False, concat=concat)\n",
    "# \t\t#self.conv2 = SAGEConv(32, out_channels, normalize=False, concat=concat)\n",
    "# \t\tself.conv1 = SAGEConv(in_channels, 32, normalize=False)\n",
    "# \t\tself.conv2 = SAGEConv(32, out_channels, normalize=False)\n",
    "\n",
    "\n",
    "# \tdef forward(self, x, data_flow):\n",
    "# \t\tdata = data_flow[0]\n",
    "# \t\tx = x[data.n_id]\n",
    "# \t\tx = F.relu(self.conv1((x, None), data.edge_index, size=data.size,res_n_id=data.res_n_id))\n",
    "# \t\tx = F.dropout(x, p=0.5, training=self.training)\n",
    "# \t\tdata = data_flow[1]\n",
    "# \t\tx = self.conv2((x, None), data.edge_index, size=data.size,res_n_id=data.res_n_id)\n",
    "\n",
    "# \t\treturn F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "class SAGENet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SAGENet, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = F.relu(self.conv1((x, x_target), edge_index, size=size))\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## Original ###############################################\n",
    "# def train():\n",
    "# \tmodel.train()\n",
    "# \ttotal_loss = 0\n",
    "# \tfor data_flow in loader(data.train_mask):\n",
    "# \t\toptimizer.zero_grad()\n",
    "# \t\tout = model(data.x.to(device), data_flow.to(device))\n",
    "# \t\tloss = F.nll_loss(out, data.y[data_flow.n_id].to(device))\n",
    "# \t\tloss.backward()\n",
    "# \t\toptimizer.step()\n",
    "# \t\ttotal_loss += loss.item() * data_flow.batch_size\n",
    "# \treturn total_loss / data.train_mask.sum().item()\n",
    "# #######################################################################\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in loader:\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x[n_id].to(device), adjs)\n",
    "        loss = F.nll_loss(out, data.y[n_id[:batch_size]].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_size\n",
    "    return total_loss / len(data.train_mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def train():\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     train_mask_long = data.train_mask.nonzero(as_tuple=False).view(-1).long()\n",
    "#     loader = NeighborSampler(data.edge_index, node_idx=train_mask_long, sizes=[-1], batch_size=32, shuffle=True, num_workers=4)\n",
    "#     for data_flow_tuple in loader:\n",
    "#         print(\"Data flow tuple:\", data_flow_tuple)\n",
    "#         data_flow = data_flow_tuple[0]\n",
    "#         print(\"Data flow:\", data_flow)\n",
    "#         optimizer.zero_grad()\n",
    "#         edge_index = data_flow.edge_index.to(device)\n",
    "#         out = model(data.x.to(device), edge_index)\n",
    "#         loss = F.binary_cross_entropy_with_logits(out[data_flow.n_id[-1]], data.y[data_flow.n_id[-1]].to(device))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(data.train_mask.nonzero(as_tuple=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(mask):\n",
    "\tmodel.eval()\n",
    "\tcorrect = 0\n",
    "\tfor data_flow in loader(mask):\n",
    "\t\tout = model(data.x.to(device), data_flow.to(device))\n",
    "\t\tpred = out.max(1)[1]\n",
    "\t\tpro  = F.softmax(out, dim=1)\n",
    "\t\tpro1 = pro.max(1)\n",
    "\t\tfor i in range(len(data_flow.n_id)):\n",
    "\t\t\tpro[i][pro1[1][i]] = -1\n",
    "\t\tpro2 = pro.max(1)\n",
    "\t\tfor i in range(len(data_flow.n_id)):\n",
    "\t\t\tif pro1[0][i]/pro2[0][i] < thre:\n",
    "\t\t\t\tpred[i] = 100\n",
    "\t\tcorrect += pred.eq(data.y[data_flow.n_id].to(device)).sum().item()\n",
    "\treturn correct / mask.sum().item()\n",
    "\n",
    "def final_test(mask):\n",
    "\tmodel.eval()\n",
    "\tcorrect = 0\n",
    "\tfor data_flow in loader(mask):\n",
    "\t\tout = model(data.x.to(device), data_flow.to(device))\n",
    "\t\tpred = out.max(1)[1]\n",
    "\t\tpro  = F.softmax(out, dim=1)\n",
    "\t\tpro1 = pro.max(1)\n",
    "\t\tfor i in range(len(data_flow.n_id)):\n",
    "\t\t\tpro[i][pro1[1][i]] = -1\n",
    "\t\tpro2 = pro.max(1)\n",
    "\t\tfor i in range(len(data_flow.n_id)):\n",
    "\t\t\tif pro1[0][i]/pro2[0][i] < thre:\n",
    "\t\t\t\tpred[i] = 100\n",
    "\t\tfor i in range(len(data_flow.n_id)):\n",
    "\t\t\tif data.y[data_flow.n_id[i]] != pred[i]:\n",
    "\t\t\t\tfp.append(int(data_flow.n_id[i]))\n",
    "\t\t\telse:\n",
    "\t\t\t\ttn.append(int(data_flow.n_id[i]))\n",
    "\t\tcorrect += pred.eq(data.y[data_flow.n_id].to(device)).sum().item()\n",
    "\treturn correct / mask.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e5caa42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "\tglobal fp, tn\n",
    "\tglobal loader, device, model, optimizer, data\n",
    "\n",
    "\tshow('Start validating')\n",
    "\tpath = 'graphchi-cpp-master/graph_data/darpatc/' + args.scene + '_test.txt'\n",
    "\tdata, feature_num, label_num, adj, adj2, nodeA, _nodeA, _neibor = MyDatasetA(path, 0)\n",
    "\tdataset = TestDatasetA(data)\n",
    "\tdata = dataset[0]\n",
    "\tprint(data)\n",
    "\tloader = NeighborSampler(data, sizes=[1, 1], num_hops=2, batch_size=b_size, shuffle=False, add_self_loops=True)\n",
    "\tdevice = torch.device('cpu')\t\n",
    "\tNet = SAGENet\t\n",
    "# \tmodel1 = Net(feature_num, label_num).to(device)\n",
    "\tmodel1 = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "\tmodel = model1\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\tfp = []\n",
    "\ttn = []\n",
    "\n",
    "\tout_loop = -1\n",
    "\twhile(1):\n",
    "\t\tout_loop += 1\n",
    "\t\tprint('validating in model ', str(out_loop))\n",
    "\t\tmodel_path = 'models/model_'+str(out_loop)\n",
    "\t\tif not osp.exists(model_path): break\n",
    "\t\tmodel.load_state_dict(torch.load(model_path))\n",
    "\t\tfp = []\n",
    "\t\ttn = []\n",
    "\t\tauc = final_test(data.test_mask)\n",
    "\t\tprint('fp and fn: ', len(fp), len(tn))\n",
    "\t\t_fp = 0\n",
    "\t\t_tp = 0\n",
    "\t\teps = 1e-10\n",
    "\t\ttempNodeA = {}\n",
    "\t\tfor i in nodeA:\n",
    "\t\t\ttempNodeA[i] = 1\n",
    "\t\tfor i in fp:\n",
    "\t\t\tif not i in _nodeA:\n",
    "\t\t\t\t_fp += 1\n",
    "\t\t\tif not i in _neibor.keys():\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tfor j in _neibor[i]:\n",
    "\t\t\t\tif j in tempNodeA.keys():\n",
    "\t\t\t\t\ttempNodeA[j] = 0\n",
    "\t\tfor i in tempNodeA.keys():\n",
    "\t\t\tif tempNodeA[i] == 0:\n",
    "\t\t\t\t_tp += 1\n",
    "\t\tprint('Precision: ', _tp/(_tp+_fp))\n",
    "\t\tprint('Recall: ', _tp/len(nodeA))\n",
    "\t\tif (_tp/len(nodeA) > 0.8) and (_tp/(_tp+_fp+eps) > 0.7):\n",
    "\t\t\twhile (1):\n",
    "\t\t\t\tout_loop += 1\n",
    "\t\t\t\tmodel_path = 'models/model_'+str(out_loop)\n",
    "\t\t\t\tif not osp.exists(model_path): break\n",
    "\t\t\t\tprint(\"123\")                   \n",
    "# \t\t\t\tos.system('rm ../models/model_'+str(out_loop))\n",
    "# \t\t\t\tos.system('rm ../models/tn_feature_label_'+str(graphId)+'_'+str(out_loop)+'.txt')\n",
    "# \t\t\t\tos.system('rm ../models/fp_feature_label_'+str(graphId)+'_'+str(out_loop)+'.txt')\n",
    "\t\t\treturn 1\n",
    "\t\tif (_tp/len(nodeA) <= 0.8):\n",
    "\t\t\treturn 0\n",
    "\t\tfor j in tn:\n",
    "\t\t\tdata.test_mask[j] = False\n",
    "\t\t\n",
    "\treturn 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "421c068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import add_self_loops\n",
    "edge_index_with_self_loops, _ = add_self_loops(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef6dfda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pro():\n",
    "\tglobal data, nodeA, _nodeA, _neibor, b_size, feature_num, label_num, graphId\n",
    "\tglobal model, loader, optimizer, device, fp, tn, loop_num\n",
    "\tos.system('python setup.py')\n",
    "\tpath = 'graphchi-cpp-master/graph_data/darpatc/' + args.scene + '_train.txt'\n",
    "\tgraphId = 0\n",
    "\tshow('Start training graph ' + str(graphId))\n",
    "\tdata1, feature_num, label_num, adj, adj2 = MyDataset(path, 0)\n",
    "\tdataset = TestDataset(data1)\n",
    "\tdata = dataset[0]\n",
    "\tprint(data)\n",
    "\tprint('feature ', feature_num, '; label ', label_num)\n",
    "\tedge_index_with_self_loops, _ = add_self_loops(data.edge_index)\n",
    "# \tloader = NeighborSampler(data, sizes=[1.0, 1.0], num_hops=2, batch_size=b_size, shuffle=False, add_self_loops=True)\n",
    "\tloader = NeighborSampler(data.edge_index, sizes=[1, 1], batch_size=b_size, shuffle=False)\n",
    "\t#loader = NeighborSampler(data, sizes=[1.0, 1.0], num_hops=2, batch_size=b_size, shuffle=False, add_self_loops=True)\n",
    "\t#loader = NeighborSampler(data.edge_index, sizes=[1.0, 1.0], num_hops=2, batch_size=b_size, shuffle=False, add_self_loops=True)\n",
    "\n",
    "###############################\n",
    "# def train_pro():\n",
    "# \tglobal data, nodeA, _nodeA, _neibor, b_size, feature_num, label_num, graphId\n",
    "# \tglobal model, loader, optimizer, device, fp, tn, loop_num\n",
    "# \tos.system('python setup.py')\n",
    "# \tpath = 'graphchi-cpp-master/graph_data/darpatc/' + args.scene + '_train.txt'\n",
    "# \tgraphId = 0\n",
    "# \tshow('Start training graph ' + str(graphId))\n",
    "# \tdata1, feature_num, label_num, adj, adj2 = MyDataset(path, 0)\n",
    "# \tdataset = TestDataset(data1)\n",
    "# \tdata = dataset[0]\n",
    "# \tprint(data)\n",
    "# \tprint('feature ', feature_num, '; label ', label_num)\n",
    "# \tb_size = 512  # Define batch size\n",
    "# \tloader = NeighborSampler(data.edge_index, sizes=[1, 1], batch_size=b_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tdevice = torch.device('cpu')\n",
    "\tNet = SAGENet\n",
    "\tmodel = Net(feature_num, label_num).to(device)\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\tfor epoch in range(1, 30):\n",
    "\t\tloss = train()\n",
    "\t\tauc = test(data.test_mask)\n",
    "\t\tshow(epoch, loss, auc)\n",
    "\n",
    "\tloop_num = 0\n",
    "\tmax_thre = 3\n",
    "\tbad_cnt = 0\n",
    "\twhile (1):\n",
    "\t\tfp = []\n",
    "\t\ttn = []\n",
    "\t\tauc = final_test(data.test_mask)\n",
    "\t\tif len(tn) == 0:\n",
    "\t\t\tbad_cnt += 1\n",
    "\t\telse:\n",
    "\t\t\tbad_cnt = 0\n",
    "\t\tif bad_cnt >= max_thre:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tif len(tn) > 0:\n",
    "\t\t\tfor i in tn:\n",
    "\t\t\t\tdata.train_mask[i] = False\n",
    "\t\t\t\tdata.test_mask[i] = False\n",
    "\n",
    "\n",
    "\t\t\tfw = open('models/fp_feature_label_'+str(graphId)+'_'+str(loop_num)+'.txt', 'w')\n",
    "\t\t\tx_list = data.x[fp]\n",
    "\t\t\ty_list = data.y[fp]\n",
    "\t\t\tprint(len(x_list))\n",
    "\t\t\t\n",
    "\t\t\tif len(x_list) >1:\n",
    "\t\t\t\tsorted_index = np.argsort(y_list, axis = 0)\n",
    "\t\t\t\tx_list = np.array(x_list)[sorted_index]\n",
    "\t\t\t\ty_list = np.array(y_list)[sorted_index]\n",
    "\n",
    "\t\t\tfor i in range(len(y_list)):\n",
    "\t\t\t\tfw.write(str(y_list[i])+':')\n",
    "\t\t\t\tfor j in x_list[i]:\n",
    "\t\t\t\t\tfw.write(' '+str(j))\n",
    "\t\t\t\tfw.write('\\n')\n",
    "\t\t\tfw.close()\n",
    "\n",
    "\t\t\tfw = open('models/tn_feature_label_'+str(graphId)+'_'+str(loop_num)+'.txt', 'w')\n",
    "\t\t\tx_list = data.x[tn]\n",
    "\t\t\ty_list = data.y[tn]\n",
    "\t\t\tprint(len(x_list))\n",
    "\t\t\t\n",
    "\t\t\tif len(x_list) >1:\n",
    "\t\t\t\tsorted_index = np.argsort(y_list, axis = 0)\n",
    "\t\t\t\tx_list = np.array(x_list)[sorted_index]\n",
    "\t\t\t\ty_list = np.array(y_list)[sorted_index]\n",
    "\n",
    "\t\t\tfor i in range(len(y_list)):\n",
    "\t\t\t\tfw.write(str(y_list[i])+':')\n",
    "\t\t\t\tfor j in x_list[i]:\n",
    "\t\t\t\t\tfw.write(' '+str(j))\n",
    "\t\t\t\tfw.write('\\n')\n",
    "\t\t\tfw.close()\n",
    "\t\t\ttorch.save(model.state_dict(),'models/model_'+str(loop_num))\n",
    "\t\t\tloop_num += 1\n",
    "\t\t\tif len(fp) == 0: break\n",
    "\t\tauc = 0\n",
    "\t\tfor epoch in range(1, 150):\n",
    "\t\t\tloss = train()\n",
    "\t\t\tauc = test(data.test_mask)\n",
    "\t\t\tshow(epoch, loss, auc)\n",
    "\t\t\tif loss < 1: break\n",
    "\tshow('Finish training graph ' + str(graphId))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ff0b6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "# \tglobal b_size, args, thre\n",
    "# \tparser = argparse.ArgumentParser()\n",
    "# \tparser.add_argument('--model', type=str, default='SAGE')\n",
    "# \tparser.add_argument('--scene', type=str, default='theia')\n",
    "# \targs = parser.parse_args()\n",
    "# \tassert args.model in ['SAGE']\n",
    "# \tassert args.scene in ['cadets','trace','theia','fivedirections']\n",
    "# \tb_size = 5000\n",
    "# \tthre = thre_map[args.scene]\n",
    "# \tos.system('cp groundtruth/'+args.scene+'.txt groundtruth_uuid.txt')\n",
    "# \twhile (1):\n",
    "# \t\ttrain_pro()\n",
    "# \t\tflag = validate()\n",
    "# \t\tif flag == 1:\n",
    "# \t\t\tbreak\n",
    "# \t\telse:\n",
    "# \t\t\tprint(\"Done\")\n",
    "# \t\t\tos.system('rm ../models/model_*')\n",
    "# \t\t\tos.system('rm ../models/tn_feature_label_*')\n",
    "# \t\t\tos.system('rm ../models/fp_feature_label_*')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3a1fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "# \tgraphchi_root = os.path.abspath(os.path.join(os.getcwd(), 'graphchi-cpp-master'))\n",
    "# \tos.environ['GRAPHCHI_ROOT'] = graphchi_root\n",
    "\t\n",
    "# \tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9d20f1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training graph 0 2023-05-06 19:39:30\n",
      "Data(edge_index=[2, 9388492], test_mask=[304973], train_mask=[304973], x=[304973, 36], y=[304973])\n",
      "feature  36 ; label  5\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-4d9967c92d36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-4d9967c92d36>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp groundtruth/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt groundtruth_uuid.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain_pro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-f94034dc04ea>\u001b[0m in \u001b[0;36mtrain_pro\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-d779f86a1362>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0madjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madjs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/log3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/log3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "sys.argv = ['train_darpatc.py', '--graphchi_root', '/path/to/graphchi', '--model', 'SAGE', '--scene', 'theia']\n",
    "\n",
    "def main():\n",
    "    global b_size, args, thre\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--graphchi_root', type=str, default='')\n",
    "    parser.add_argument('--model', type=str, default='SAGE')\n",
    "    parser.add_argument('--scene', type=str, default='theia')\n",
    "    args = parser.parse_args()\n",
    "    assert args.model in ['SAGE']\n",
    "    assert args.scene in ['cadets','trace','theia','fivedirections']\n",
    "    b_size = 5000\n",
    "    thre = thre_map[args.scene]\n",
    "    os.environ['GRAPHCHI_ROOT'] = args.graphchi_root\n",
    "    os.system('cp groundtruth/'+args.scene+'.txt groundtruth_uuid.txt')\n",
    "    while (1):\n",
    "        train_pro()\n",
    "        flag = validate()\n",
    "        if flag == 1:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Done\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f3d1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec87be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log3",
   "language": "python",
   "name": "log3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
